{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-01T19:15:18.003730Z","iopub.execute_input":"2023-06-01T19:15:18.004652Z","iopub.status.idle":"2023-06-01T19:15:18.010985Z","shell.execute_reply.started":"2023-06-01T19:15:18.004607Z","shell.execute_reply":"2023-06-01T19:15:18.009827Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:15:18.269059Z","iopub.execute_input":"2023-06-01T19:15:18.269720Z","iopub.status.idle":"2023-06-01T19:15:18.274071Z","shell.execute_reply.started":"2023-06-01T19:15:18.269684Z","shell.execute_reply":"2023-06-01T19:15:18.272957Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"class CNNBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=3,\n                 stride=1,\n                 padding=0):\n        super(CNNBlock, self).__init__()\n        \n        self.seq_block = nn.Sequential(\n            nn.Conv2d(in_channels,out_channels,kernel_size,stride, padding, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n        \n    def forward(self, x):\n        x= self.seq_block(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:15:18.276564Z","iopub.execute_input":"2023-06-01T19:15:18.277395Z","iopub.status.idle":"2023-06-01T19:15:18.288115Z","shell.execute_reply.started":"2023-06-01T19:15:18.277362Z","shell.execute_reply":"2023-06-01T19:15:18.287177Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"class CNNBlocks(nn.Module):\n    \"\"\"\n    Parameters:\n    n_conv (int): creates a block of n_conv convolutions\n    in_channels (int): number of in_channels of the first block's convolution\n    out_channels (int): number of out_channels of the first block's convolution\n    expand (bool) : if True after the first convolution of a blocl the number of channels doubles\n    \"\"\"\n    def __init__(self,\n                 n_conv,\n                 in_channels,\n                 out_channels,\n                 padding):\n        super(CNNBlocks, self).__init__()\n        self.layers = nn.ModuleList()\n        for i in range(n_conv):\n\n            self.layers.append(CNNBlock(in_channels, out_channels, padding=padding))\n            # after each convolution we set (next) in_channel to (previous) out_channels\n            in_channels = out_channels\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:15:18.290826Z","iopub.execute_input":"2023-06-01T19:15:18.291138Z","iopub.status.idle":"2023-06-01T19:15:18.300945Z","shell.execute_reply.started":"2023-06-01T19:15:18.291114Z","shell.execute_reply":"2023-06-01T19:15:18.300006Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \"\"\"\n    Parameters:\n    in_channels (int): number of in_channels of the first CNNBlocks\n    out_channels (int): number of out_channels of the first CNNBlocks\n    padding (int): padding applied in each convolution\n    downhill (int): number times a CNNBlocks + MaxPool2D it's applied.\n    \"\"\"\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 padding,\n                 downhill=4):\n        super(Encoder, self).__init__()\n        self.enc_layers = nn.ModuleList()\n\n        for _ in range(downhill):\n            self.enc_layers += [\n                    CNNBlocks(n_conv=2, in_channels=in_channels, out_channels=out_channels, padding=padding),\n                    nn.MaxPool2d(2, 2)\n                ]\n\n            in_channels = out_channels\n            out_channels *= 2\n        # doubling the dept of the last CNN block\n        self.enc_layers.append(CNNBlocks(n_conv=2, in_channels=in_channels,\n                                         out_channels=out_channels, padding=padding))\n        \n    def forward(self, x):\n        route_connection = []\n        for layer in self.enc_layers:\n            if isinstance(layer, CNNBlocks):\n                x = layer(x)\n                route_connection.append(x)\n            else:\n                x = layer(x)\n        return x, route_connection","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:15:18.302575Z","iopub.execute_input":"2023-06-01T19:15:18.302873Z","iopub.status.idle":"2023-06-01T19:15:18.316632Z","shell.execute_reply.started":"2023-06-01T19:15:18.302842Z","shell.execute_reply":"2023-06-01T19:15:18.315607Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"\nclass Decoder(nn.Module):\n    \"\"\"\n    Parameters:\n    in_channels (int): number of in_channels of the first ConvTranspose2d\n    out_channels (int): number of out_channels of the first ConvTranspose2d\n    padding (int): padding applied in each convolution\n    uphill (int): number times a ConvTranspose2d + CNNBlocks it's applied.\n    \"\"\"\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 exit_channels,\n                 padding,\n                 uphill=4):\n        super(Decoder, self).__init__()\n        self.exit_channels = exit_channels\n        self.layers = nn.ModuleList()\n\n        for i in range(uphill):\n\n            self.layers += [\n                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n                CNNBlocks(n_conv=2, in_channels=in_channels,\n                          out_channels=out_channels, padding=padding),\n            ]\n            in_channels //= 2\n            out_channels //= 2\n\n        # cannot be a CNNBlock because it has ReLU incorpored\n        # cannot append nn.Sigmoid here because you should be later using\n        # BCELoss () which will trigger the amp error \"are unsafe to autocast\".\n        self.layers.append(\n            nn.Conv2d(in_channels, exit_channels, kernel_size=1, padding=padding),\n        )\n\n    def forward(self, x, routes_connection):\n        # pop the last element of the list since\n        # it's not used for concatenation\n        routes_connection.pop(-1)\n        for layer in self.layers:\n            if isinstance(layer, CNNBlocks):\n                # center_cropping the route tensor to make width and height match\n                routes_connection[-1] = center_crop(routes_connection[-1], x.shape[2])\n                # concatenating tensors channel-wise\n                x = torch.cat([x, routes_connection.pop(-1)], dim=1)\n                x = layer(x)\n            else:\n                x = layer(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:15:18.469857Z","iopub.execute_input":"2023-06-01T19:15:18.470210Z","iopub.status.idle":"2023-06-01T19:15:18.480975Z","shell.execute_reply.started":"2023-06-01T19:15:18.470181Z","shell.execute_reply":"2023-06-01T19:15:18.479602Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"class UNET(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 first_out_channels,\n                 exit_channels,\n                 downhill,\n                 padding=0\n                 ):\n        super(UNET, self).__init__()\n        self.encoder = Encoder(in_channels, first_out_channels, padding=padding, downhill=downhill)\n        self.decoder = Decoder(first_out_channels*(2**downhill), first_out_channels*(2**(downhill-1)),\n                               exit_channels, padding=padding, uphill=downhill)\n\n    def forward(self, x):\n        enc_out, routes = self.encoder(x)\n        out = self.decoder(enc_out, routes)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:15:18.483252Z","iopub.execute_input":"2023-06-01T19:15:18.483854Z","iopub.status.idle":"2023-06-01T19:15:18.496457Z","shell.execute_reply.started":"2023-06-01T19:15:18.483818Z","shell.execute_reply":"2023-06-01T19:15:18.495576Z"},"trusted":true},"execution_count":62,"outputs":[]}]}